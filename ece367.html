<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>ECE 367: Matrix Algebra and Optimization (Fall 2024)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">Main</a></div>
<div class="menu-item"><a href="about.html">About</a></div>
<div class="menu-category">Publications</div>
<div class="menu-item"><a href="pubs.html">Publications</a></div>
<div class="menu-item"><a href="master.html">Master&nbsp;Thesis</a></div>
<div class="menu-item"><a href="phdthesis.html">Ph.D.&nbsp;Thesis</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="ece367.html" class="current">ECE&nbsp;367</a></div>
<div class="menu-category">Miscellaneous</div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="contact.html">Contact</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>ECE 367: Matrix Algebra and Optimization (Fall 2024)</h1>
</div>
<h2>Course Description</h2>
<p>This course provides students with a grounding in optimization methods and the matrix algebra upon which they are based. The first past of the course focuses on fundamental building blocks in linear algebra and their geometric interpretation: matrices, their use to represent data and as linear operators, and the matrix decompositions (such as eigen-, spectral-, and singular-vector decompositions) that reveal structural and geometric insight. The second part of the course focuses on optimization, both unconstrained and constrained, linear and non-linear, as well as convex and nonconvex; conditions for local and global optimality, as well as basic classes of optimization problems are discussed. Applications from machine learning, signal processing, and engineering are used to illustrate the techniques developed.</p>
<h2>Personnel</h2>
<h3><b>Instructor</b>:</h3>
<ul>
<li><p>Nadim Ghaddar &lt;nadim.ghaddar@utoronto.ca&gt;</p>
</li>
<li><p>Office hours: Tuesday 12:00 pm - 1:00 pm in BA8180 (or by appointment)</p>
</li>
</ul>
<h3><b>Teaching Assistants</b>:</h3>
<ul>
<li><p>Kareem Attiah &lt;kareem.attiah@mail.utoronto.ca&gt;</p>
</li>
<li><p>Adnan Hamida &lt;adnan.hamida@mail.utoronto.ca&gt;</p>
</li>
<li><p>Faeze Moradi Kalarde &lt;faeze.moradi@mail.utoronto.ca&gt;</p>
</li>
<li><p>Nick Kwan &lt;nick.kwan@mail.utoronto.ca&gt;</p>
</li>
</ul>
<h3><b>Lectures</b>: (Starting September 3rd, 2024)</h3>
<ul>
<li><p>Monday 6:00 pm - 7:00 pm in <b>BA 1170</b></p>
</li>
<li><p>Tuesday 10:00 am - 12:00 pm in <b>BA 1190</b></p>
</li>
</ul>
<h3><b>Tutorials</b>: (Starting September 6th, 2024)</h3>
<ul>
<li><p>Friday 9:00 am - 11:00 am in (<b>WB 219</b>, <b>BA 2159</b>, <b>WB 119</b>)</p>
</li>
</ul>
<h2>Textbooks</h2>
<ol>
<li><p>Giuseppe Calafiore and Laurent El Ghaoui, <i>Optimization Models</i>, Cambridge University Press, 2014. (Main textbook)</p>
</li>
<li><p>Stephen Boyd and Lieven Vandenberghe, <i>Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares</i>, Cambridge University Press, 2018. (Some homework problems are taken from this textbook. PDF available at authorsâ€™ website.)</p>
</li>
</ol>
<h2>Course Schedule</h2>
<p>The course (roughly) follows the following schedule.<br /></p>
<table id="SCHEDULE">
<tr class="r1"><td class="c1"><b>Week</b>  </td><td class="c2"> <b>Topics</b> </td><td class="c3"> <b>Text References</b> </td><td class="c4"> <b>Assessment</b> </td><td class="c5"> <b>Tutorial</b> </td></tr>
<tr class="r2"><td class="c1">Sept 3 </td><td class="c2"> Vectors, Norms, Inner Products </td><td class="c3"> Ch. 2.1-2.2 </td><td class="c4">   </td><td class="c5">  </td></tr>
<tr class="r3"><td class="c1">Sept 9-10  </td><td class="c2"> Orthogonal Decomposition, Projection onto Subspaces, Gram-Schmidt, <br /> QR decomposition, Hyperplanes and Half-Spaces  </td><td class="c3"> Ch. 2.2-2.3 </td><td class="c4"> Homework #1: Due Sept 24, 11:59pm <br /> (Word Vector, Fourier Series) </td><td class="c5"> Homework #1: <br /> Theory </td></tr>
<tr class="r4"><td class="c1">Sept 16-17 </td><td class="c2"> Non-Euclidean Projection, Projection onto Affine Sets, <br /> Functions, Gradients and Hessians </td><td class="c3"> Ch. 2.3-2.4 </td><td class="c4">  </td><td class="c5"> Homework #1: <br /> Applications </td></tr>
<tr class="r5"><td class="c1">Sept 23-24 </td><td class="c2"> Matrices, Range, Null Space, Eigenvalues, <br /> Eigenvectors, Matrix Diagonalization </td><td class="c3"> Ch. 3.1-3.5 </td><td class="c4">  Homework #2: Due Oct 8, 11:59pm <br /> (Function Approximation, PageRank) </td><td class="c5"> Homework #2: <br /> Theory </td></tr>
<tr class="r6"><td class="c1">Sept 30 -<br />  Oct 1 </td><td class="c2"> Symmetric matrices, Orthogonal Matrices, Spectral Decomposition, <br /> Positive Semidefinite Matrices, Ellipsoids </td><td class="c3"> Ch. 4.1-4.4 </td><td class="c4">  </td><td class="c5"> Homework #2: <br /> Applications </td></tr>
<tr class="r7"><td class="c1">Oct 7-8 </td><td class="c2"> Singular Value Decomposition, <br /> Principal Component Analysis </td><td class="c3"> Ch. 5.1, 5.3.2 </td><td class="c4"> Homework #3: Due Oct 22, 11:59pm <br /> (Latent Semantic Indexing, EigenFace) </td><td class="c5"> Homework #3: <br /> Theory </td></tr>
<tr class="r8"><td class="c1">Oct 15 </td><td class="c2"> Interpretations of SVD </td><td class="c3"> Ch. 5.2 </td><td class="c4">  </td><td class="c5"> Homework #3: <br /> Applications </td></tr>
<tr class="r9"><td class="c1">Oct 21-22 </td><td class="c2"> Low-Rank Approximation, <br /> Midterm Review </td><td class="c3"> Ch. 5.3.1 </td><td class="c4">  </td><td class="c5"> Previous <br /> Midterm </td></tr>
<tr class="r10"><td class="c1">Oct 28-29 </td><td class="c2"> <b>Study Break,</b> <br /> <b>No Classes</b> </td><td class="c3">   </td><td class="c4">   </td><td class="c5">   </td></tr>
<tr class="r11"><td class="c1">Nov 4-5  </td><td class="c2"> Least Squares, Overdetermined and <br /> Underdetermined Linear Equations </td><td class="c3"> Ch. 6.1-6.4 </td><td class="c4"> Homework #4: Due Nov 19, 11:59pm <br /> (Optimal Control, CAT Scan) </td><td class="c5"> Homework #4: <br /> Theory </td></tr>
<tr class="r12"><td class="c1">Nov 11-12 </td><td class="c2"> Regularized Least-Squares, <br /> Convex Sets and Convex Functions </td><td class="c3"> Ch. 6.7.3,<br /> Ch. 8.1-8.4 </td><td class="c4">   </td><td class="c5"> Homework #4: <br /> Applications </td></tr>
<tr class="r13"><td class="c1">Nov 18-19 </td><td class="c2"> Lagrangian Method for Constrained Optimization, <br /> Linear Programming and Quadratic Programming </td><td class="c3"> Ch. 8.5, <br /> Ch. 9.1-9.6 </td><td class="c4"> Homework #5: Due Dec 3, 11:59pm <br /> (Portfolio Design, Sparse Coding of Image) </td><td class="c5"> Homework #5: <br /> Theory </td></tr>
<tr class="r14"><td class="c1">Nov 25-26 </td><td class="c2"> Numerical Algorithms for Unconstrained <br /> and Constrained Optimization </td><td class="c3"> Ch. 12.1-12.3 </td><td class="c4">   </td><td class="c5"> Homework #5: <br /> Applications </td></tr>
<tr class="r15"><td class="c1">Dec 2-3 </td><td class="c2"> Revision </td><td class="c3">  </td><td class="c4">  </td><td class="c5">  Previous <br /> Final
</td></tr></table>
<h2>Grading</h2>
<ul>
<li><p>Homework: 20%</p>
</li>
<li><p>Midterm exam: 35%</p>
</li>
<li><p>Final exam: 45%</p>
</li>
</ul>
<h2>Syllabus</h2>
<p>A pdf version of the syllabus that includes all the details can be downloaded <a href="docs/ECE367H1F-2024_Fall_Syllabus.pdf">here</a>.</p>
</td>
</tr>
</table>
</body>
</html>
