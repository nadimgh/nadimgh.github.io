# jemdoc: menu{MENU}{mlprojs.html}, nofooter
= Machine Learning

In my time at Microsoft Research and at IIT Kanpur, I have had the chance to explore both practical and interesting problems in Machine Learning \n \n

- *Leveraging Distributional Semantics for Multi-Label Learning* \n In this project, I worked on the problem of designing embedding based multi-label learning algorithms using distributional semantics which has more efficient training procedures. We extended the approach to naturally incorporate other sources of side-information, in particular the label-label co-occurrence matrix. (Joint work with Rahul Wadbude, IIT Kanpur, Piyush Rai, IIT Kanpur, Nagararjan Natararjan, Microsoft Research Lab, India & Harish Karnick, IIT Kanpur)
- *Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles* \n In this project, I worked on the problem of efficient Estimation of generalization error for ensembles using normality assumption on classification scores. We worked on efficient prediction of accuracy, ensemble parameters, bias and variance of generalization errors using minimal number of ensembles. (Work done as part of summer internship under Sundararajan Sellamanickam)
- *Bayes Optimal Classication for Hierarchy* \n In this project, I worked on the problem of finding bayes optimal classifier for Hierarchical classfication for assymetric loss. Showed under reasonable assumption over hierarchy that the Bayes optimal classification for this asymmetric loss can be found in O(log(n)). Currently extending the consistency of hierarchical classification algorithm on asymmetric tree distance loss
using calibrated surrogates. (Joint work with Dheeraj Mekala, IIT Kanpur, Purushottam Kar, IIT Kanpur & Harish Karnick, IIT Kanpur).
- *Resource Constrained Semi-Supervised Learning* \n In this project, I worked on semi-supervised setting where only a small subset of training examples is provided along with lot's of unlabeled data. We utilize label propagation which uses nearest neighbor algorithm on distance based graph between examples to propagate labels. Our objective was to develop a K-Nearest Neighbors algorithm earning models with fewer and sparse candidate points in each class called prototypes. The work was an extension of recent work done by our group [http://harsha-simhadri.org/EdgeML EdgeML]. This work finds applications in agriculture, industry and health care, that will benefit from inexpensive smart edge devices.
- *Predictive Maintenance using Machine Learning in Industrial Setting* \n In this project, I worked on problem of detecting anomalous downtime in industrial machines using Machine Learning. We develop multiple models on AzureML to predict and save machine downtimes. Our overall objective was to detect significant anomaly (larger downtime anomaly) while keeping false alarms within a limited budget. Our model was demoed to top leadership and multiple Customers. Currently we are looking on cost sensitive i.e. downtime and lookahead sensitive learning based anomaly detection. Apart from this we are also looking for neural seq to seq based model compression. We plan to submit this work in Machine Learning, Analytics and Data Science (MLADS, 2018) conference.