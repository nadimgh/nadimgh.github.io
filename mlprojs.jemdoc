# jemdoc: menu{MENU}{mlprojs.html}, nofooter
= Machine Learning

In my time at Microsoft Research and at IIT Kanpur, I have had the chance to explore both practical and interesting problems in Machine Learning \n \n

- *Leveraging Distributional Semantics for Multi-Label Learning* \n In this project, I worked on the problem of designing embedding based multi-label learning algorithms using distributional semantics which has more efficient training procedures. We extended the approach to naturally incorporate other sources of side-information, in particular the label-label co-occurrence matrix. (Joint work with Rahul Wadbude, IIT Kanpur, Piyush Rai, IIT Kanpur, Nagararjan Natararjan, Microsoft Research Lab, India & Harish Karnick, IIT Kanpur)
- *Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles* \n In this project, I worked on the problem of efficient Estimation of generalization error for ensembles using normality assumption on classification scores. We worked on efficient prediction of accuracy, ensemble parameters, bias and variance of generalization errors using minimal number of ensembles. (Work done as part of summer internship under Sundararajan Sellamanickam)
- *Bayes Optimal Classication for Hierarchy* \n In this project, I worked on the problem of finding bayes optimal classifier for Hierarchical classfication for assymetric loss. Showed under reasonable assumption over hierarchy that the Bayes optimal classification for this asymmetric loss can be found in O(log(n)). Currently extending the consistency of hierarchical classification algorithm on asymmetric tree distance loss
using calibrated surrogates. (Joint work with Dheeraj Mekala, IIT Kanpur, Purushottam Kar, IIT Kanpur & Harish Karnick, IIT Kanpur)